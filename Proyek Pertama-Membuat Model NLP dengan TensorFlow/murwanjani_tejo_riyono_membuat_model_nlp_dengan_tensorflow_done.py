# -*- coding: utf-8 -*-
"""Murwanjani_Tejo_Riyono-Membuat_Model_NLP_dengan_TensorFlow-Done.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18-SN_F9Tf8vH1qZddDz3uB_bhueoyqaY
"""

print ('Proyek Proyek Pertama : Membuat Model NLP dengan TensorFlow')
print ('Dataset Terlampir Bersama File zip')
print ('SMS Spam Collection Dataset : https://www.kaggle.com/uciml/sms-spam-collection-dataset')

#Data Diri
print ('Nama  : Murwanjani Tejo Riyono')
print ('Kelas : MLFE M1')

import zipfile
local_zip = '/content/SMS_Spam_Collection_Dataset.zip' #Variable local_zip berisi direktori lokasi file zip disimpan

#Proses ekstrak file zip
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/SMS_Spam_Collection_Dataset')
zip_ref.close()

base_dir = '/content/SMS_Spam_Collection_Dataset/spam.csv' #Menentukan main/base direktori

import pandas as pd
import chardet

#Proses read file csv, merubah data menjadi dataframe dan menghapus kolom yang tidak diperlukan
with open(base_dir, 'rb') as rawdata:
    result = chardet.detect(rawdata.read(100000))
result
df = pd.read_csv(base_dir,encoding='ISO-8859-1')
df= df.drop(columns='Unnamed: 2')
df= df.drop(columns='Unnamed: 3')
df= df.drop(columns='Unnamed: 4')
df.info()

#Cek Data
df.head() #Menampilkan 5 data awal

df.tail() #Menampilkan 5 data akhir

#Import tensorflow dan test split library
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Proses one-hot-encoding karena data kategorikal dan membuat dataframe baru
category = pd.get_dummies(df.v1)
df_new = pd.concat([df, category], axis=1)
df_new = df_new.drop(columns='v1')

#Cek
df_new #Menampilkan 5 data awal&akhir pada dataframe baru

#Mengubah nilai yang ada pada dataframe ke dalam tipe data numpy array menggunakan atribut values
teks = df_new['v2'].values
label = df_new[['ham', 'spam']].values

#Membagi data train dan data testing
teks_latih, teks_test, label_latih, label_test = train_test_split(teks, label, test_size=0.2)

#Proses tokenization dan konversi sample menjadi sequence
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(teks_latih) 
tokenizer.fit_on_texts(teks_test)
 
sekuens_latih = tokenizer.texts_to_sequences(teks_latih)
sekuens_test = tokenizer.texts_to_sequences(teks_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

#Setting model (layer,Embedding, LSTM) dan fungsi compile
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

#Melatih model sebanyak 10x
model.fit(padded_latih, 
          label_latih, 
          epochs=10, 
          validation_data=(padded_test, label_test), 
          verbose=2)